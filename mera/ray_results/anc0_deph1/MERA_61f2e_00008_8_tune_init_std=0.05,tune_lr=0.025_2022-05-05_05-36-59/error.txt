Failure # 1 (occurred at 2022-05-05_05-37-53)
Traceback (most recent call last):
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trial_runner.py", line 739, in _process_trial
    results = self.trial_executor.fetch_result(trial)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py", line 746, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 82, in wrapper
    return func(*args, **kwargs)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/worker.py", line 1621, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ResourceExhaustedError): [36mray::MERA.train_buffered()[39m (pid=3476, ip=10.128.0.8, repr=<model_tuning.MERA object at 0x7f0f65089490>)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trainable.py", line 178, in train_buffered
    result = self.train()
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trainable.py", line 237, in train
    result = self.step()
  File "/home/qc_whaley/dephased_ttn_project/mera/non_distributed_tunning/model_tuning.py", line 117, in step
    self.model.run_epoch(batch_size, self.iteration, grad_accumulation=config['data']['grad_accumulation'])
  File "/home/qc_whaley/dephased_ttn_project/mera/non_distributed_tunning/model_tuning.py", line 99, in run_epoch
    self.network.update(train_image_batch, train_label_batch, epoch, apply_grads=False)
  File "/home/qc_whaley/dephased_ttn_project/mera/network.py", line 113, in update
    loss = self.loss(input_batch, label_batch)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py", line 885, in __call__
    result = self._call(*args, **kwds)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py", line 956, in _call
    return self._concrete_stateful_fn._call_flat(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py", line 1971, in _call_flat
    flat_outputs = forward_function.call(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py", line 591, in call
    outputs = execute.execute(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[125,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2] and type complex64 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node StatefulPartitionedCall/einsum_26/Einsum_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__forward_loss_6838]

Function call stack:
loss

