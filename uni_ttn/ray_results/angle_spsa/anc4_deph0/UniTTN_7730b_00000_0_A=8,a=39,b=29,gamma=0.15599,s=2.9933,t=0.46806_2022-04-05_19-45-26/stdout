/home/haoranliao/dephased_ttn_project/uni_ttn/ray_results/angle_spsa/anc4_deph0/UniTTN_7730b_00000_0_A=8,a=39,b=29,gamma=0.15599,s=2.9933,t=0.46806_2022-04-05_19-45-26
Physical GPUs: 1 Logical GPUs: 1
Load Data From File
Train Sample Size: 5000
No Validation
Test Sample Size: 1902
Using 8x8 Pixel Dict
> /tmp/pycharm_project_948/uni_ttn/tf2.7/non_distributed_tunning/model.py(192)run_epoch()
-> self.network.update(train_image_batch, train_label_batch, epoch, apply_grads=True, counter=counter)
(Pdb) 0
(Pdb) (250, 64, 2)
(Pdb) (250, 2)
(Pdb) *** TypeError: update() got multiple values for argument 'apply_grads'
(Pdb) *** tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[32,8000,32768] and type complex64 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node StatefulPartitionedCall/einsum_3/Einsum}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 [Op:__forward_loss_2162]

Function call stack:
loss
(Pdb) *** tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[32,8000,32768] and type complex64 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node StatefulPartitionedCall/einsum_3/Einsum}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 [Op:__forward_loss_2162]

Function call stack:
loss
(Pdb) 