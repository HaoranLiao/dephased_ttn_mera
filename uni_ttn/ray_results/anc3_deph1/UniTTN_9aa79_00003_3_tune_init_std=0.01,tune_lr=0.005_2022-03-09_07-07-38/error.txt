Failure # 1 (occurred at 2022-03-09_07-08-22)
Traceback (most recent call last):
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trial_runner.py", line 739, in _process_trial
    results = self.trial_executor.fetch_result(trial)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py", line 746, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 82, in wrapper
    return func(*args, **kwargs)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/worker.py", line 1621, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ResourceExhaustedError): [36mray::UniTTN.train_buffered()[39m (pid=105268, ip=10.128.0.8, repr=<model.UniTTN object at 0x7f5119909310>)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trainable.py", line 178, in train_buffered
    result = self.train()
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trainable.py", line 237, in train
    result = self.step()
  File "/home/qc_whaley/dephased_ttn_project/uni_ttn/tf2.7/non_distributed_tunning/model.py", line 223, in step
    test_accuracy = self.model.run_network(self.model.test_images, self.model.test_labels, batch_size*self.model.b_factor)
  File "/home/qc_whaley/dephased_ttn_project/uni_ttn/tf2.7/non_distributed_tunning/model.py", line 162, in run_network
    pred_probs = self.network.get_network_output(image_batch)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py", line 885, in __call__
    result = self._call(*args, **kwds)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py", line 924, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py", line 3039, in __call__
    return graph_function._call_flat(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py", line 591, in call
    outputs = execute.execute(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[1000,32,16,16,16,16] and type complex64 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node einsum_4/Einsum_1 (defined at /dephased_ttn_project/uni_ttn/tf2.7/non_distributed_tunning/network.py:175) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_get_network_output_13399]

Errors may have originated from an input operation.
Input Source operations connected to node einsum_4/Einsum_1:
 strided_slice_1 (defined at /dephased_ttn_project/uni_ttn/tf2.7/non_distributed_tunning/network.py:173)

Function call stack:
get_network_output

