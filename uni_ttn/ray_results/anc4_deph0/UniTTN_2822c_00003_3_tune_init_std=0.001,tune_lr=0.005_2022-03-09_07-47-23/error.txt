Failure # 1 (occurred at 2022-03-09_07-48-09)
Traceback (most recent call last):
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trial_runner.py", line 739, in _process_trial
    results = self.trial_executor.fetch_result(trial)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py", line 746, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 82, in wrapper
    return func(*args, **kwargs)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/worker.py", line 1621, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ResourceExhaustedError): [36mray::UniTTN.train_buffered()[39m (pid=143885, ip=10.128.0.8, repr=<model.UniTTN object at 0x7f6d022fa2e0>)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trainable.py", line 178, in train_buffered
    result = self.train()
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/ray/tune/trainable.py", line 237, in train
    result = self.step()
  File "/home/qc_whaley/dephased_ttn_project/uni_ttn/tf2.7/non_distributed_tunning/model.py", line 222, in step
    self.model.run_epoch(batch_size, grad_accumulation=config['data']['grad_accumulation'])
  File "/home/qc_whaley/dephased_ttn_project/uni_ttn/tf2.7/non_distributed_tunning/model.py", line 188, in run_epoch
    self.network.update(train_image_batch, train_label_batch, apply_grads=False)
  File "/home/qc_whaley/dephased_ttn_project/uni_ttn/tf2.7/non_distributed_tunning/network.py", line 92, in update
    loss = self.loss(input_batch, label_batch)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py", line 885, in __call__
    result = self._call(*args, **kwds)
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py", line 956, in _call
    return self._concrete_stateful_fn._call_flat(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py", line 1971, in _call_flat
    flat_outputs = forward_function.call(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/function.py", line 591, in call
    outputs = execute.execute(
  File "/home/qc_whaley/anaconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[32,160,32768] and type complex64 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node StatefulPartitionedCall/einsum_3/Einsum}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__forward_loss_2157]

Function call stack:
loss

